# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/metrics.ipynb.

# %% auto 0
__all__ = ['accuracy_score', 'precision_recall_fscore', 'precision_score', 'recall_score', 'f1_score', 'log_loss', 'roc_curve',
           'roc_auc_score', 'RocCurveDisplay']

# %% ../nbs/metrics.ipynb 3
from typing import Union

import numpy as np
import matplotlib.pyplot as plt

# %% ../nbs/metrics.ipynb 4
def accuracy_score(y_true: np.ndarray, # true labels 
                   y_pred: np.ndarray, # predicted labels
                   normalize=False, # if weights is not None, normalize by sum of weights
                   weights: Union[np.ndarray,None] = None, # weights for each sample
                   ):
    """computes accuracy for binary or multiclass classification"""
    scores = (y_true==y_pred).astype(float) # bool*int, int*float not allowed
    if weights is not None: scores *= weights 
    scale = weights.sum() if weights is not None else scores.size
    return scores.sum()/scale.astype(float) if normalize else scores.sum()

# %% ../nbs/metrics.ipynb 7
def precision_recall_fscore(y_true, y_pred):
    labels = np.union1d(y_true, y_pred)
    recs = np.zeros(labels.size)
    pres = np.zeros(labels.size)
    for i, label in enumerate(labels):
        tp = (y_true==label).astype(float) @ (y_pred==label)
        fp = (y_true!=label).astype(float) @ (y_pred==label)
        fn = (y_true==label).astype(float) @ (y_pred!=label)
        recs[i] = tp/(tp+fn)
        pres[i] = tp/(tp+fp)
    fs = 2*recs*pres/(recs+pres)
    return pres.mean(), recs.mean(), fs.mean()

# %% ../nbs/metrics.ipynb 8
def precision_score(y_true, y_pred):
    """compute the average precision, even in the binary case"""
    prec, _, _ = precision_recall_fscore(y_true, y_pred)
    return prec

# %% ../nbs/metrics.ipynb 9
def recall_score(y_true, y_pred):
    """compute the average recall, even in the binary case"""
    _, rec, _ = precision_recall_fscore(y_true, y_pred)
    return rec

# %% ../nbs/metrics.ipynb 10
def f1_score(y_true, y_pred):
    """compute the average f1, even in the binary case"""
    _, _, f1 = precision_recall_fscore(y_true, y_pred)
    return f1

# %% ../nbs/metrics.ipynb 12
def log_loss(y_true, y_pred, *, sample_weights=None):
    """ y_true.dim == 1, y_pred.dim == 2, sample_weights.dim == 1"""
    probs = y_pred[np.arange(y_true.size),y_true]
    if sample_weights is not None: 
        loss = -np.log(probs) @ sample_weights / sample_weights.sum()
    else: 
        loss = -np.log(probs).mean()
    return loss

# %% ../nbs/metrics.ipynb 14
def roc_curve(y_true, y_score, *, pos_label=None):
    y_true = y_true.reshape(-1,1)
    if y_score.ndim == 1:
        y_score = np.concatenate([1-y_score.reshape(-1,1), y_score.reshape(-1,1)], axis=-1)
    y = np.concatenate([y_true, y_score], axis=-1)
    labels = np.sort(np.unique(y_true)) # assume all labels are present in y_true
    tprs = np.zeros(y_score.shape)
    fprs = np.zeros(y_score.shape)
    thresholds = np.zeros(y_score.shape)
    for i in range(len(labels)):
        idxs = np.argsort(y, axis=0)[::-1,i+1] # idxs that sort arr by score descending
        yi = np.where(y[idxs,0]==labels[i], 1, 0) # rewrite y_true to binary
        tcum = np.cumsum(yi)
        tprs[:,i] = tcum/tcum[-1]
        fcum = np.cumsum(1-yi)
        fprs[:,i] = fcum/fcum[-1]
        thresholds[:,i] = y[idxs,i+1]
    if pos_label is None:
        # assume {-1,1} or {0,1} labels in y_true
        pos_label = 1
    idx = np.where(labels==pos_label)[0].item()
    return fprs[:,idx], tprs[:,idx], thresholds[:,idx]

# %% ../nbs/metrics.ipynb 17
def roc_auc_score(y_true, y_score):
    fpr, tpr, _ =  roc_curve(y_true, y_score)
    fpr_inc = (np.roll(fpr,-1) - fpr)[:-1]
    auc = (fpr_inc * tpr[:-1]).sum()
    return auc

# %% ../nbs/metrics.ipynb 20
class RocCurveDisplay:
    """plot result of `roc_curve` which returns fpr, tpr, _ """
    @classmethod
    def from_predictions(cls, y_true, y_score):
        fpr, tpr, _ = roc_curve(y_true, y_score)
        return plt.plot(fpr, tpr)
    
    @classmethod
    def from_estimator(cls, clf, y_true, y_score):
        raise NotImplemented
