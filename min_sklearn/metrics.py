# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/metrics.ipynb.

# %% auto 0
__all__ = ['accuracy_score', 'confusion_matrix', 'precision_recall_fscore', 'precision_score', 'recall_score', 'f1_score',
           'log_loss', 'roc_curve', 'roc_auc_score', 'RocCurveDisplay', 'ConfusionMatrixDisplay']

# %% ../nbs/metrics.ipynb 3
from typing import Union

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# %% ../nbs/metrics.ipynb 4
def accuracy_score(y_true: np.ndarray, # true labels 
                   y_pred: np.ndarray, # predicted labels
                   normalize=False, # if weights is not None, normalize by sum of weights
                   sample_weight: Union[np.ndarray,None] = None, # weights for each sample
                   ):
    """computes accuracy for binary or multiclass classification"""
    scores = (y_true==y_pred).astype(float) # bool*int, int*float not allowed
    if sample_weight is not None: scores *= sample_weight 
    scale = sample_weight.sum() if sample_weight is not None else scores.size
    return scores.sum()/scale.astype(float) if normalize else scores.sum()

# %% ../nbs/metrics.ipynb 7
def confusion_matrix(
        y_true: np.ndarray, # true labels
        y_pred: np.ndarray, # predicted labels
        sample_weight: Union[np.ndarray,None] = None, # weights for each sample
):
    """compute confusion matrix for multiclass classification;
    assume all labels are present and range from 0 to n_classes-1;
    $C_{ij}$ is the number of samples that have true label $i$ and predicted label $j$
    
    """
    labels = np.union1d(y_true, y_pred)
    # one hot encode the labels then use matrix multiplication to count things
    ohe_true = np.eye(labels.size)[y_true]
    ohe_pred = np.eye(labels.size)[y_pred] 
    # handle weights
    if sample_weight is not None:
        ohe_true = ohe_true*sample_weight[:,None]
    C = ohe_true.T @ ohe_pred
    return C

# %% ../nbs/metrics.ipynb 9
def precision_recall_fscore(y_true:np.ndarray,# true labels 
                            y_pred:np.ndarray, # predicted labels
                            sample_weight: Union[np.ndarray,None] = None, # weights for each sample
                            ):
    """macro average the metrics across classes"""
    labels = np.union1d(y_true, y_pred)
    recs = np.zeros(labels.size)
    pres = np.zeros(labels.size)
    w = sample_weight if sample_weight is not None else np.ones(y_true.size)
    for i, label in enumerate(labels):
        tp = (y_true==label).astype(float)*w @ (y_pred==label)
        fp = (y_true!=label).astype(float)*w @ (y_pred==label)
        fn = (y_true==label).astype(float)*w @ (y_pred!=label)
        recs[i] = tp/(tp+fn) # assume no division by zero
        pres[i] = tp/(tp+fp)
    fs = 2*recs*pres/(recs+pres)
    return pres.mean(), recs.mean(), fs.mean()

# %% ../nbs/metrics.ipynb 10
def precision_score(y_true, y_pred, **kwargs):
    """compute the macro averaged precision """
    prec, _, _ = precision_recall_fscore(y_true, y_pred, **kwargs)
    return prec

# %% ../nbs/metrics.ipynb 11
def recall_score(y_true, y_pred, **kwargs):
    """compute the macro averaged recall"""
    _, rec, _ = precision_recall_fscore(y_true, y_pred, **kwargs)
    return rec

# %% ../nbs/metrics.ipynb 12
def f1_score(y_true, y_pred, **kwargs):
    """compute the macro averaged f1 score"""
    _, _, f1 = precision_recall_fscore(y_true, y_pred, **kwargs)
    return f1

# %% ../nbs/metrics.ipynb 14
def log_loss(y_true, y_pred, *, sample_weight=None):
    """ y_true.dim == 1, y_pred.dim == 2, sample_weights.dim == 1"""
    probs = y_pred[np.arange(y_true.size),y_true]
    if sample_weight is not None: 
        loss = -np.log(probs) @ sample_weight / sample_weight.sum()
    else: 
        loss = -np.log(probs).mean()
    return loss

# %% ../nbs/metrics.ipynb 16
def roc_curve(y_true, y_score, *, pos_label=None):
    """inherently binary. `pos_label` can be any label in y_true. 
    compute the ROC curve for that label.
    use a different convention for the accumulated counts to sklearn that leads to the same auc score"""
    y_true = y_true.reshape(-1,1)
    if y_score.ndim == 1:
        y_score = np.concatenate([1-y_score.reshape(-1,1), y_score.reshape(-1,1)], axis=-1)
    y = np.concatenate([y_true, y_score], axis=-1)
    labels = np.sort(np.unique(y_true)) # assume all labels are present in y_true
    tprs = np.zeros(y_score.shape)
    fprs = np.zeros(y_score.shape)
    thresholds = np.zeros(y_score.shape)
    for i in range(len(labels)):
        idxs = np.argsort(y, axis=0)[::-1,i+1] # idxs that sort arr by score descending
        yi = np.where(y[idxs,0]==labels[i], 1, 0) # rewrite y_true to binary
        tcum = np.cumsum(yi)
        tprs[:,i] = tcum/tcum[-1]
        fcum = np.cumsum(1-yi)
        fprs[:,i] = fcum/fcum[-1]
        thresholds[:,i] = y[idxs,i+1]
    if pos_label is None:
        # assume {-1,1} or {0,1} labels in y_true
        pos_label = 1
    idx = np.where(labels==pos_label)[0].item()
    return fprs[:,idx], tprs[:,idx], thresholds[:,idx]

# %% ../nbs/metrics.ipynb 18
def roc_auc_score(y_true, y_score):
    fpr, tpr, *_ =  roc_curve(y_true, y_score)
    fpr_inc = (np.roll(fpr,-1) - fpr)[:-1]
    auc = (fpr_inc * tpr[:-1]).sum()
    return auc

# %% ../nbs/metrics.ipynb 20
class RocCurveDisplay:
    """plot result of `roc_curve` which returns fpr, tpr, _ """
    @classmethod
    def from_predictions(cls, y_true, y_score):
        fpr, tpr, *_ = roc_curve(y_true, y_score)
        return plt.plot(fpr, tpr)
    
    @classmethod
    def from_estimator(cls, clf, y_true, y_score):
        raise NotImplemented

# %% ../nbs/metrics.ipynb 22
class ConfusionMatrixDisplay:
    """plot result of `confusion_matrix`"""
    @classmethod
    def from_predictions(cls, y_true, y_pred, sample_weight=None):
        C = confusion_matrix(y_true, y_pred, sample_weight)
        sns.heatmap(C, cmap='Blues', annot=True)
        plt.xlabel('Predicted')
        plt.ylabel('True')
        plt.show()
